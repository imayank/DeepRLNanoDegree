{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The algorithm used to solve the environment is Multi Agent Deep Deterministic Policy Gradients as described [here](https://arxiv.org/pdf/1706.02275.pdf).\n",
    "\n",
    "The algorithm adpats the actor-critic architecture of Deep Determimistic Policy Gradients to make it work for environments comprising of more than one agent. The adaptation presented can be used in various scenarios of multi agent setting, for example competitive, cooperative etc.\n",
    "\n",
    "The adaptation uses framework of centralized training with decentralized execution. Centralized training means that while training the critic can use extra information(like actions taken by other agents) to ease training process. Decentralized execution makes sure that while testing each actor uses only its local observation to predict actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OverviewArchitecture](./OverviewArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Initialize two actor-critic pairs for the two agents`.\n",
    "  - `Each actor has two networks: Local`$\\mu^{\\theta}$ `and target`$\\mu^{\\theta^{'}}$ `network with parameters` $\\theta$ `and` $\\theta^{'}$.\n",
    "  - `Each critic has two networks: Local` $Q^{\\phi}$ `and target` $Q^{\\phi^{'}}$ `network with parameters` $\\phi$ `and` $\\phi^{'}$.\n",
    "  \n",
    "2. `Initialize Replay Memory` $\\mathcal{D}$.\n",
    "3. `Initialize noise process` $\\mathcal{N}_{t}$ `for each agent`.\n",
    "4. `for episode = 1 to M:`\n",
    "  - `Reset environment and get initial state` $s$\n",
    "  - `Untill the episode is complete do:`\n",
    "    - `if episode <= 500 select random actions for agents:` $a_{t}\\sim StandardNormal(0,1)$ `Otherwise: ` $a_{t} = \\mu^{\\theta}(o_{t}) + \\mathcal{N}_{t}$ `where` $o_{t}$ `is local observation to agent t`\n",
    "    - `Execute actions` $a=(a_{1},a_{2})$ `and observe reward` $r$ `and new state` $s^{'}$\n",
    "    - `Store` $(s,a,r,s^{'})$ `in` $\\mathcal{D}$\n",
    "    - $s = s^{'}$\n",
    "    - `if len`$(\\mathcal{D})$ ` > UPDATE_AFTER then for agent = 1 to 2 do:` \n",
    "      - `Sample a uniformly random mini-batch` $\\mathcal{B}$ `from` $\\mathcal{D}$.   $\\mathcal{B}= (s^{j},a^{j},r^{j},s^{'j})$  \n",
    "      - `Set` $target_{t} = r^{j}_{t} + \\gamma Q^{\\phi^{'}}(s^{'j},a_{1}^{'},a_{2}^{'})\\mid_{a_{t}^{'}=\\mu^{\\theta^{'}}(o_{t}^{j})}$\n",
    "      - `Update critic by minimizing the huber loss:` $HuberLoss(target_{t},Q^{\\phi}(s^{j},a_{1}^{j},a_{2}^{j}) )$\n",
    "      - `Update the actor using sampled policy gradient: ` $\\bigtriangledown_{\\theta_{t}}J \\approx \\frac{1}{B} \\sum_{j} \\bigtriangledown_{\\theta_{t}}\\mu_{t}(o_{t}^{j})\\bigtriangledown_{a_{t}}Q^{\\phi}(s^{j},a_{1}^{j},a_{2}^{j})\\mid_{a_{t}^{j}=\\mu^{\\theta}(o_{t}^{j})}$\n",
    "    - `Update traget network parameters:`\n",
    "      - $\\theta^{'} = \\tau \\theta + (1-\\tau)\\theta^{'}$\n",
    "      - $\\phi^{'} = \\tau \\phi + (1-\\tau)\\phi^{'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "- BUFFER_SIZE = $10^{6}$\n",
    "   - Size of the replay buffer\n",
    "- BATCH_SIZE = 128 \n",
    "   - number of instances sampled in a batch\n",
    "- GAMMA = 0.99\n",
    "   - Discount factor\n",
    "- TAU = $10^{-3}$ \n",
    "   - Parameter for soft update of the target network parameters\n",
    "- LR_ACTOR = $10^{-3}$\n",
    "   - Learning rate for actor network \n",
    "- LR_QNET = $10^{-4}$\n",
    "   - Learning rate for critic network\n",
    "- WEIGHT_DECAY = 0\n",
    "   - L2 weight decay parameters for network layers\n",
    "- NOISE_SCALE = 0.1\n",
    "   - scaling parameter of the noise added to actions\n",
    "- UPDATE_AFTER = 1000\n",
    "   - Minimum number of samples in the buffer after which learning can start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Architecture\n",
    "\n",
    "- Critic Netwoek takes as input states of the two agents and actions predicted using the target networks of two agents as input. \n",
    "- Individual State is of size 24, so state from two agents becomes a tensor of size (48, ) that becomes input to the first linear layer.\n",
    "- Input to the Second linear layer is a concatenated tensor. Action tensor from the first actor is concatenated with the output of first_linear_layer + ReLU block.\n",
    "- Similarly the input to third linear layer is a concatenated tensor. Action tensor from the second actor is concatenated with the output of second_linear_layer + ReLU block.\n",
    "- Output is a single value.\n",
    "\n",
    "Detailed arcitecture if given below\n",
    "\n",
    "1. **linear1**: Linear(in_features=48, out_features=512, bias=True)\n",
    "2. **ReLU** layer\n",
    "3. **Concatenation**: Concatenate output from `linear1 + ReLU` layer to action tensor of size (2,). This action is from first actor\n",
    "4. **linear2**: Linear(in_features=514, out_features=256, bias=True)\n",
    "5. **ReLU** layer\n",
    "6. **Concatenation**: Concatenate output from `linear2 + ReLU` layer to action tensor of size (2,). This action is from second actor\n",
    "7. **linear3**: Linear(in_features=258, out_features=256, bias=True)\n",
    "8. **ReLU** layer\n",
    "9. **linear4**: Linear(in_features=256, out_features=128, bias=True)\n",
    "10. **ReLU** layer\n",
    "11. **linear5**: Linear(in_features=128, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CriticArchitecture](./critic-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Architecture\n",
    "\n",
    "An actor takes its local state as input (tensor of size 24) and outputs a tensor of size (2,)\n",
    "\n",
    "Detailed architecture is given below:\n",
    "\n",
    "1. **linear1**: Linear(in_features=24, out_features=128, bias=True)\n",
    "2. **ReLU** layer\n",
    "3. **linear2**: Linear(in_features=128, out_features=2, bias=True)\n",
    "4. **activation**: Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ActorArchitecture](./actor-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the Rewards\n",
    "\n",
    "**The environment is solved in 1600 episodes**\n",
    "\n",
    "![Result](./result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "- **Priority Buffer**: Incorprating Replay memory with priority based sampling instead of uniform sampling. It might speed up the learning.\n",
    "- **Tuning Hyperparameters**: Trying different setting of hyper-parameters and presenting how changing them have impact on learning time or if the model diverges for some setting.\n",
    "- **Ensemble of actors**: As mentioned in the [paper](https://arxiv.org/pdf/1706.02275.pdf), single policy might overfit to the behaviour of the other agent so it might produce better results if ensemble of policy is used.\n",
    "- **Inferring  Policies of other agent**: also mentioned in the [paper](https://arxiv.org/pdf/1706.02275.pdf), Instead of using the actions predicted by other agents keep an approximation of their policy and learn this approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained agents playing\n",
    "![AgentPlaying](./sample_play.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprlnd",
   "language": "python",
   "name": "deeprlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
