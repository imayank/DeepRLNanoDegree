{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The algorithm used to solve the environment is Double Deep Q learning as presented [here](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "\n",
    "The [Deep Q learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) uses multi-layered neural network to approximate the state-action value function $\\mathcal{Q}$. This network takes state $s$ as input and provides apporximation to $\\mathcal{Q(s,a)}$ for all the discrete actions $a$.   \n",
    "To stablize the training process it introduces two very important ideas:\n",
    "- **Replay Memory**: All the one step interactions( or multi-step if multistep returns is used) are stored in a memory called replay buffer/memory. To update network a batch of transitions is sampled uniformly from this buffer after sufficient number of transitions are available in the replay buffer. This step randomizes over data and helps to remove the correlations present in sequentially gathered data.\n",
    "- **Target Network**: A target network is same as online deep Q network, except that its parameters are copied from the online network after fixed period time $\\tau$. Between periods its parameters remain fixed. Thus it reduces correlation with target value.\n",
    "\n",
    "If $\\theta$ represents local online network parameters and $\\theta^{'}$ target network parameters then the loss function at iteration $i$ is:   \n",
    "$\\mathcal{L}_{i}(\\theta_{i}) = \\mathbb{E}_{(s,a,r,s{'})\\sim\\mathcal{U(D)}}\\lbrack(r + \\gamma max_{a_{'}}\\mathcal{Q}(s^{'},a^{'};\\theta^{'}_{i}) - \\mathcal{Q}(s,a;\\theta_{i}))^{2}\\rbrack$   \n",
    "\n",
    "The max operator in Deep Q Learning is used for both selecting an action and evaluatinf its value. It has been [shown](https://arxiv.org/pdf/1509.06461.pdf) that this results in overestimation of state-action values $\\mathcal{Q}$. Overestimation of state-action value can result in sub-optimal performance/learning. This over-estimation can be avoided by seperating the selection of an action and evaluating corresponding state-action value. This is the idea behind Double Deep Q learning.   \n",
    "\n",
    "In double deep Q learing the local online Q network is used for selecting an action value and the target network is used for evaluating  corresponding state-action value. This small change helps greatly in reducing the over-estimation of state-action values.   \n",
    "The loss for Double deep Q learning is:   \n",
    "$\\mathcal{L}_{i}(\\theta_{i}) = \\mathbb{E}_{(s,a,r,s{'})\\sim\\mathcal{U(D)}}\\lbrack(r + \\gamma\\mathcal{Q}(s^{'},a_{max};\\theta^{'}_{i}) - \\mathcal{Q}(s,a;\\theta_{i}))^{2}\\rbrack$   \n",
    "where, $a_{max} = argmax_{a^{'}} \\mathcal{Q}({s^{'},a^{'};\\theta_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Initialize a local Q-network and a target Q-network`.\n",
    "  - `local Q-network parameters` $\\theta$ `and target Q-network parameters` $\\theta^{'}$. `Initially` $\\theta^{'} = \\theta$\n",
    "2. `Initialize a Replay Buffer` $\\mathcal{D}$.\n",
    "3. `Intialize the start value of epsilon` $\\epsilon = 1$\n",
    "4. `for episode = 1 to M:`\n",
    "  - `Reset environment and get initial state` $s$\n",
    "  - `set value of step` $step = 0$\n",
    "  - `Untill the episode is complete do:`\n",
    "    - `An agent chooses an action as follows:`\n",
    "      - $a_{max} = argmax_{a^{'}} \\mathcal{Q}(s,a^{'};\\theta)$   \n",
    "      `with probability` $1-\\epsilon, a = a_{max}$    \n",
    "      `with probability`$\\epsilon, a = \\mathcal{U}(0,1,2,3)$\n",
    "    - `Perform action` $a$ `and obtain reward` $r$ `and next state` $s^{'}$\n",
    "    - `Store `$(s,a,r,s^{'})$ `in` $\\mathcal{D}$\n",
    "    - $s = s^{'}$\n",
    "    - `update step value ` $step = (step + 1) \\% $ `UPDATE-EVERY` \n",
    "    - `if len`$(\\mathcal{D})$ ` > UPDATE_AFTER and` $step == 0$   \n",
    "      - `Sample a mini-batch ` $\\mathcal{B}$ `from` $\\mathcal{D}$.   \n",
    "      $\\mathcal{B}= (s^{j},a^{j},r^{j},s^{'j})$    \n",
    "      `where,` $s^{j}$: `states`    \n",
    "      $a^{j}$: `actions`    \n",
    "      $r^{j}$: `rewards`    \n",
    "      $s^{'j}$: `next states`    \n",
    "      - `Set` $target= r^{j} + \\gamma\\mathcal{Q}(s^{'j},a_{max}^{j};\\theta^{'})$   \n",
    "      `where, ` $a_{max}^{j} = argmax_{a^{'}} \\mathcal{Q}({s^{'j},a^{'};\\theta})$\n",
    "      - `Update local Q-network by minimizing the following loss:` $\\frac{1}{B} \\sum_{j}(target - Q(s^{j},a^{j};\\theta))^{2}$\n",
    "      - `Update target network parameters:`\n",
    "      - $\\theta^{'} = \\tau \\theta + (1-\\tau)\\theta^{'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "- BUFFER_SIZE = $10^{5}$\n",
    "   - Size of the replay buffer\n",
    "- BATCH_SIZE = 64 \n",
    "   - number of instances sampled in a batch\n",
    "- GAMMA = 0.99\n",
    "   - Discount factor\n",
    "- TAU = $10^{-3}$ \n",
    "   - Parameter for soft update of the target network parameters\n",
    "- LR = $5\\times10^{-4}$\n",
    "   - Learning rate for critic network\n",
    "- UPDATE_AFTER = 64\n",
    "   - Minimum number of samples in the buffer after which learning can start\n",
    "- UPDATE_EVERY = 4\n",
    "   - Network updation period\n",
    "- $\\epsilon_{start}$ = 1.0\n",
    "   - initial value of $\\epsilon$\n",
    "- $\\epsilon_{end}$ = 0.01\n",
    "   - Final value of $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "- Q-Network takes state $s$ of an agent as its input. It outputs $\\mathcal{Q}(s,a)$ for all $a \\in \\{0,1,2,3\\}$  \n",
    "- Individual State is of size 37, so a tensor of shape (33,)becomes input to the first linear layer(fc1) of size 64. It is followed by a ReLU unit.\n",
    "- Second linear layer(fc2) is of size 64. It takes output of `fc1 + ReLU` as input. It is followed by a ReLU layer.\n",
    "- Third linear layer(fc3) is of size 4, It takes output of `fc2 + ReLU` as input. It is final layer of the network.\n",
    "\n",
    "Detailed arcitecture is given below\n",
    "\n",
    "1. **fc1**: Linear(in_features=37, out_features=64, bias=True)\n",
    "2. **ReLU** layer\n",
    "3. **fc2**: Linear(in_features=64, out_features=64, bias=True)\n",
    "4. **ReLU** layer\n",
    "5. **fc3**: Linear(in_features=64, out_features=4, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NetworkArchitecture](./network-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the Rewards\n",
    "\n",
    "**The environment is solved in 382 episodes**\n",
    "\n",
    "![Result](./result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "- **Tuning Hyperparameters**: Trying different setting of hyper-parameters and presenting how changing them have impact on learning time or if the model diverges for some setting.\n",
    "- **Priority Replay Buffer**: Using replay memory with priority based sampling. It might speed up the training.\n",
    "- **Dueling Architecture**: Using dueling architecture for solving the environment and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained agents playing\n",
    "![AgentPlaying](./trained_agent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./sample_play.gif \"Trained Agent\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./sample_play.gif \"Trained Agent\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprlnd",
   "language": "python",
   "name": "deeprlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
