{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The algorithm used to solve the environment is Double Deep Q learning as presented [here](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "\n",
    "The [Deep Q learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) uses multi-layered neural network to approximate the state-action value function $\\mathcal{Q}$. This network takes state $s$ as input and provides apporximation to $\\mathcal{Q(s,a)}$ for all the discrete actions $a$.   \n",
    "To stablize the training process it introduces two very important ideas:\n",
    "- **Replay Memory**: All the one step interactions( or multi-step if multistep returns is used) are stored in a memory called replay buffer/memory. To update network a batch of transitions is sampled uniformly from this buffer after sufficient number of transitions are available in the replay buffer. This step randomizes over data and helps to remove the correlations present in sequentially gathered data.\n",
    "- **Target Network**: A target network is same as online deep Q network, except that its parameters are copied from the online network after fixed period time $\\tau$. Between periods its parameters remain fixed. Thus it reduces correlation with target value.\n",
    "\n",
    "If $\\theta$ represents local online network parameters and $\\theta^{'}$ target network parameters then the loss function at iteration $i$ is:   \n",
    "$\\mathcal{L}_{i}(\\theta_{i}) = \\mathbb{E}_{(s,a,r,s{'})\\sim\\mathcal{U(D)}}\\lbrack(r + \\gamma max_{a_{'}}\\mathcal{Q}(s^{'},a^{'};\\theta^{'}_{i}) - \\mathcal{Q}(s,a;\\theta_{i}))^{2}\\rbrack$   \n",
    "\n",
    "The max operator in Deep Q Learning is used for both selecting an action and evaluatinf its value. It has been [shown](https://arxiv.org/pdf/1509.06461.pdf) that this results in overestimation of state-action values $\\mathcal{Q}$. Overestimation of state-action value can result in sub-optimal performance/learning. This over-estimation can be avoided by seperating the selection of an action and evaluating corresponding state-action value. This is the idea behind Double Deep Q learning.   \n",
    "\n",
    "In double deep Q learing the local online Q network is used for selecting an action value and the target network is used for evaluating  corresponding state-action value. This small change helps greatly in reducing the over-estimation of state-action values.   \n",
    "The loss for Double deep Q learning is:   \n",
    "$\\mathcal{L}_{i}(\\theta_{i}) = \\mathbb{E}_{(s,a,r,s{'})\\sim\\mathcal{U(D)}}\\lbrack(r + \\gamma\\mathcal{Q}(s^{'},a_{max};\\theta^{'}_{i}) - \\mathcal{Q}(s,a;\\theta_{i}))^{2}\\rbrack$   \n",
    "where, $a_{max} = argmax_{a^{'}} \\mathcal{Q}({s^{'},a^{'};\\theta_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Initialize a local Q-network and a target Q-network`.\n",
    "  - `local Q-network parameters` $\\theta$ `and target Q-network parameters` $\\theta^{'}$. `Initially` $\\theta^{'} = \\theta$\n",
    "2. `Initialize a Replay Buffer` $\\mathcal{D}$.\n",
    "3. `Intialize the start value of epsilon` $\\epsilon = 1$\n",
    "4. `for episode = 1 to M:`\n",
    "  - `Reset environment and get initial state` $s$\n",
    "  - `set value of step` $step = 0$\n",
    "  - `Untill the episode is complete do:`\n",
    "    - `An agent chooses an action as follows:`\n",
    "      - $a_{max} = argmax_{a^{'}} \\mathcal{Q}(s,a^{'};\\theta)$   \n",
    "      `with probability` $1-\\epsilon, a = a_{max}$    \n",
    "      `with probability`$\\epsilon, a = \\mathcal{U}(0,1,2,3)$\n",
    "    - `Perform action` $a$ `and obtain reward` $r$ `and next state` $s^{'}$\n",
    "    - `Store `$(s,a,r,s^{'})$ `in` $\\mathcal{D}$\n",
    "    - $s = s^{'}$\n",
    "    - `update step value ` $step = (step + 1) \\% $ `UPDATE-EVERY` \n",
    "    - `if len`$(\\mathcal{D})$ ` > UPDATE_AFTER and` $step == 0$   \n",
    "      - `Sample a mini-batch ` $\\mathcal{B}$ `from` $\\mathcal{D}$.   \n",
    "      $\\mathcal{B}= (s^{j},a^{j},r^{j},s^{'j})$    \n",
    "      `where,` $s^{j}$: `states`    \n",
    "      $a^{j}$: `actions`    \n",
    "      $r^{j}$: `rewards`    \n",
    "      $s^{'j}$: `next states`    \n",
    "      - `Set` $target= r^{j} + \\gamma\\mathcal{Q}(s^{'j},a_{max}^{j};\\theta^{'})$   \n",
    "      `where, ` $a_{max}^{j} = argmax_{a^{'}} \\mathcal{Q}({s^{'j},a^{'};\\theta})$\n",
    "      - `Update local Q-network by minimizing the following loss:` $\\frac{1}{B} \\sum_{j}(target - Q(s^{j},a^{j};\\theta))^{2}$\n",
    "      - `Update target network parameters:`\n",
    "      - $\\theta^{'} = \\tau \\theta + (1-\\tau)\\theta^{'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "- BUFFER_SIZE = $10^{5}$\n",
    "   - Size of the replay buffer\n",
    "- BATCH_SIZE = 64 \n",
    "   - number of instances sampled in a batch\n",
    "- GAMMA = 0.99\n",
    "   - Discount factor\n",
    "- TAU = $10^{-3}$ \n",
    "   - Parameter for soft update of the target network parameters\n",
    "- LR = $5\\times10^{-4}$\n",
    "   - Learning rate for critic network\n",
    "- UPDATE_AFTER = 64\n",
    "   - Minimum number of samples in the buffer after which learning can start\n",
    "- UPDATE_EVERY = 4\n",
    "   - Network updation period\n",
    "- $\\epsilon_{start}$ = 1.0\n",
    "   - initial value of $\\epsilon$\n",
    "- $\\epsilon_{end}$ = 0.01\n",
    "   - Final value of $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Architecture\n",
    "\n",
    "- Critic Network takes state of an agent and predicted action as its input. \n",
    "- Individual State is of size 33, so a tensor of shape (33,)becomes input to the first linear layer of size 256.\n",
    "- Input to the Second linear layer is a concatenated tensor. Action tensor from the input is concatenated with the output of first_linear_layer + Leaky_ReLU block. Second layer is of size 256. It is followed by a leaky ReLU layer.\n",
    "- Third layer is of size 128, it is followed by a leaky ReLU layer.\n",
    "- Final layer is of size 1.\n",
    "\n",
    "Detailed arcitecture is given below\n",
    "\n",
    "1. **linear1**: Linear(in_features=33, out_features=256, bias=True)\n",
    "2. **Leaky ReLU** layer\n",
    "3. **Concatenation**: Concatenate output from `linear1 + Leaky_ReLU` layer to action tensor of size (4, ).\n",
    "4. **linear2**: Linear(in_features=260, out_features=256, bias=True)\n",
    "5. **Leaky ReLU** layer\n",
    "6. **linear3**: Linear(in_features=256, out_features=128, bias=True)\n",
    "7. **Leaky ReLU** layer\n",
    "8. **linear4**: Linear(in_features=128, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CriticArchitecture](./critic-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Architecture\n",
    "\n",
    "An actor takesa state as input (tensor of size 33) and outputs a action tensor of size (4,)\n",
    "\n",
    "Detailed architecture is given below:\n",
    "\n",
    "1. **linear1**: Linear(in_features=33, out_features=256, bias=True)\n",
    "2. **ReLU** layer\n",
    "3. **linear2**: Linear(in_features=256, out_features=4, bias=True)\n",
    "4. **activation**: Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ActorArchitecture](./actor-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the Rewards\n",
    "\n",
    "**The environment is solved in 1300 episodes**\n",
    "\n",
    "![Result](./result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "- **Tuning Hyperparameters**: Trying different setting of hyper-parameters and presenting how changing them have impact on learning time or if the model diverges for some setting.\n",
    "- **Multi-Agent Environment**: Applying this same algorithm to Multi-Agent reacher environment with a common replay memory for training multiple agents simultaneously.\n",
    "- **Other Algorithms**: Trying PPO and A3C to solve the environment and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained agents playing\n",
    "![AgentPlaying](./sample_play.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./sample_play.gif \"Trained Agent\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./sample_play.gif \"Trained Agent\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprlnd",
   "language": "python",
   "name": "deeprlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
