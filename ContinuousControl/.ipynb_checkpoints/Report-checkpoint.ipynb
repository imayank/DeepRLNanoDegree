{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The algorithm used to solve the environment is Deep Deterministic Policy Gradients (DDPG) as described [here](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "\n",
    "The algorithm adpats the idea of actor-critic algorithm to the continuous action domain. It's an off policy algorithm which combines ideas from [Deep Q learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) and [deteministic policy gradients](http://proceedings.mlr.press/v32/silver14.pdf) to learn policies in environments that has continuous action spaces.\n",
    "\n",
    "DDPG uses two networks. One network for actor which represents current policy and maps a state to an action. Other network is for the critic which given a state action pair provides its value(state-action value) as output. As DDPG employs deep neural network to approximate state-action value and to represent policy it makes use of Replay Memory and target networks to stablize the training. Also, to help with exploration of the sapce noise is added to the output of the actor network. This noise is sampled from a noise process.\n",
    "\n",
    "To solve the Reacher environment(single agent) the following is used with the basic DDPG algorithm:\n",
    "- Replay memory with priority based sampling (Priority Replay Buffer). It is implemented using Segment Tree data structure.\n",
    "- Multi-step return is used instead of single step returns.\n",
    "\n",
    "Above two are used with the Reacher environment as the rewards are scarce and using Priority Replay buffer with multistep returns might help with the convergence as well as speed of convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Initialize an actor network and a critic network`.\n",
    "  - `An actor has two networks: Local`$\\mu^{\\theta}$ `and target`$\\mu^{\\theta^{'}}$ `network with parameters` $\\theta$ `and` $\\theta^{'}$. `Initially` $\\theta = \\theta^{'}$\n",
    "  - `A critic has two networks: Local` $Q^{\\phi}$ `and target` $Q^{\\phi^{'}}$ `network with parameters` $\\phi$ `and` $\\phi^{'}$. `Initially` $\\phi = \\phi^{'}$\n",
    "  \n",
    "2. `Initialize Priority Replay Buffer` $\\mathcal{D}$.\n",
    "3. `Initialize noise process` $\\mathcal{N}$.\n",
    "4. `for episode = 1 to M:`\n",
    "  - `Reset environment and get initial state` $s$\n",
    "  - `Untill the episode is complete do:`\n",
    "    - `From state` $s$ `obtain rewards` $r=(r_{1},r_{2},r_{3},r_{4},r_{5})$ `for 5-step returns and state` $s^{'}$ `after the final step. An agent chooses an action as follows:`\n",
    "      - `if episode <= 10:`$a\\sim StandardNormal(0,1)$  `(random action)`   \n",
    "      `Otherwise: ` $a = \\mu^{\\theta}(s) + \\mathcal{N}$\n",
    "    - `Get initial priority for `$(s,a,r,s^{'})$ ` and Store` $(s,a,r,s^{'},priority)$ `in` $\\mathcal{D}$\n",
    "    - $s = s^{'}$\n",
    "    - `if len`$(\\mathcal{D})$ ` > UPDATE_AFTER`  \n",
    "      - `Sample a mini-batch on priority basis` $\\mathcal{B}$ `from` $\\mathcal{D}$.   $\\mathcal{B}= (s^{j},a^{j},r^{j},s^{'j},w^{j})$    \n",
    "      `where,` $s^{j}$: `states`    \n",
    "      $a^{j}$: `actions`    \n",
    "      $r^{j}$: `set of 5 rewards(5 step return)`    \n",
    "      $s^{'j}$: `next states`    \n",
    "      $w^{j}$: `Importance sampling weights`\n",
    "      - `Set` $target= r^{j}_{1} + \\gamma r^{j}_{2} + \\gamma^{2} r^{j}_{3} + \\gamma^{3} r^{j}_{4} + \\gamma^{4} r^{j}_{5} + \\gamma^{5} Q^{\\phi^{'}}(s^{'j},a^{'j})$    \n",
    "      `where, `$a^{'j} = \\mu^{\\theta{'}}(s^{'j})$\n",
    "      - `Update critic by minimizing the following loss:` $w^{j}\\times(target - Q^{\\phi}(s^{j},a^{j}))^{2}$\n",
    "      - `Update the actor using sampled policy gradient: ` $\\bigtriangledown_{\\theta}J \\approx \\frac{1}{B} \\sum_{j} \\bigtriangledown_{\\theta}\\mu(s^{j})\\bigtriangledown_{a}Q^{\\phi}(s^{j},a^{j})\\mid_{a^{j}=\\mu^{\\theta}(s^{j})}$\n",
    "    - `Update traget network parameters:`\n",
    "      - $\\theta^{'} = \\tau \\theta + (1-\\tau)\\theta^{'}$\n",
    "      - $\\phi^{'} = \\tau \\phi + (1-\\tau)\\phi^{'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "- BUFFER_SIZE = $10^{6}$\n",
    "   - Size of the replay buffer\n",
    "- BATCH_SIZE = 128 \n",
    "   - number of instances sampled in a batch\n",
    "- GAMMA = 0.99\n",
    "   - Discount factor\n",
    "- TAU = $10^{-3}$ \n",
    "   - Parameter for soft update of the target network parameters\n",
    "- LR_ACTOR = $10^{-4}$\n",
    "   - Learning rate for actor network \n",
    "- LR_QNET = $3\\times10^{-4}$\n",
    "   - Learning rate for critic network\n",
    "- WEIGHT_DECAY = $10^{-4}$\n",
    "   - L2 weight decay parameters for network layers\n",
    "- NOISE_SCALE = 0.1\n",
    "   - scaling parameter of the noise added to actions\n",
    "- UPDATE_AFTER = 1000\n",
    "   - Minimum number of samples in the buffer after which learning can start\n",
    "- ALPHA = 0.6             \n",
    "  - factor controling amount of prioritization\n",
    "- REPLAY_EPS = 1e-6         \n",
    "  - added to priority to facilitate exploration\n",
    "- BETA = 1                \n",
    "  - factor controling amount of importance sampling weights decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Architecture\n",
    "\n",
    "- Critic Network takes state of an agent and predicted action as its input. \n",
    "- Individual State is of size 33, so a tensor of shape (33,)becomes input to the first linear layer of size 256.\n",
    "- Input to the Second linear layer is a concatenated tensor. Action tensor from the input is concatenated with the output of first_linear_layer + Leaky_ReLU block. Second layer is of size 256. It is followed by a leaky ReLU layer.\n",
    "- Third layer is of size 128, it is followed by a leaky ReLU layer.\n",
    "- Final layer is of size 1.\n",
    "\n",
    "Detailed arcitecture is given below\n",
    "\n",
    "1. **linear1**: Linear(in_features=33, out_features=256, bias=True)\n",
    "2. **Leaky ReLU** layer\n",
    "3. **Concatenation**: Concatenate output from `linear1 + Leaky_ReLU` layer to action tensor of size (4, ).\n",
    "4. **linear2**: Linear(in_features=260, out_features=256, bias=True)\n",
    "5. **Leaky ReLU** layer\n",
    "6. **linear3**: Linear(in_features=256, out_features=128, bias=True)\n",
    "7. **Leaky ReLU** layer\n",
    "8. **linear4**: Linear(in_features=128, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CriticArchitecture](./critic-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Architecture\n",
    "\n",
    "An actor takesa state as input (tensor of size 33) and outputs a action tensor of size (4,)\n",
    "\n",
    "Detailed architecture is given below:\n",
    "\n",
    "1. **linear1**: Linear(in_features=33, out_features=256, bias=True)\n",
    "2. **ReLU** layer\n",
    "3. **linear2**: Linear(in_features=256, out_features=4, bias=True)\n",
    "4. **activation**: Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ActorArchitecture](./actor-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the Rewards\n",
    "\n",
    "**The environment is solved in 1300 episodes**\n",
    "\n",
    "![Result](./result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "- **Tuning Hyperparameters**: Trying different setting of hyper-parameters and presenting how changing them have impact on learning time or if the model diverges for some setting.\n",
    "- **Multi-Agent Environment**: Applying this same algorithm to Multi-Agent reacher environment with a common replay memory for training multiple agents simultaneously.\n",
    "- **Other Algorithms**: Trying PPO and A3C to solve the environment and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained agents playing\n",
    "![AgentPlaying](./sample_play.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./sample_play.gif \"Trained Agent\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./sample_play.gif \"Trained Agent\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprlnd",
   "language": "python",
   "name": "deeprlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
